{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Loading: Training Dataset.\n",
    "\n",
    "fids = glob('./weather_data/*csv'); fids.sort()\n",
    "df = pd.DataFrame()\n",
    "#-- Selected Columns.\n",
    "#-- The reason why the training dataset only select the vars below is because\n",
    "#--     1. They are common vars happened in the dataset among years\n",
    "#--     2. But elimite the 'EVAP' because the numbers of nan value is varied. \n",
    "jointCols=['Lat', 'Lon', 'AWND', 'PRCP', 'SNOW', 'SNWD', \n",
    "           'TAVG', 'TMAX', 'TMIN', 'TOBS', 'WESD', 'WT01', 'WT02', \n",
    "           'WT03', 'WT04', 'WT05', 'WT06', 'WT07', 'WT08', 'WT11', 'date']\n",
    "for fid in fids:\n",
    "    year = fid.split('/')[2].split('_')[0]\n",
    "    if int(year) >= 1992 and int(year) <=2015:\n",
    "        dd = pd.read_csv(fid,header=0)\n",
    "        dd['date'] = pd.to_datetime(year+ dd.month.astype('str'), format='%Y%m')  \n",
    "        df = df.append(dd[jointCols])\n",
    "\n",
    "df = df.set_index('date')\n",
    "df.index = df.index.strftime('%Y-%m')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Loading: Target Dataset.\n",
    "conn = sqlite3.connect('./FPA_FOD_20170508.sqlite')\n",
    "query = \"SELECT * From Fires\"\n",
    "target= pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target[target['STATE'] == 'CA']\n",
    "target = target[['DISCOVERY_DOY',\n",
    "                 'DISCOVERY_DATE',\n",
    "                 'LATITUDE',\n",
    "                 'LONGITUDE',\n",
    "                 'FIRE_YEAR',\n",
    "                 'FIRE_SIZE_CLASS',\n",
    "                 'STAT_CAUSE_DESCR']].drop_duplicates().dropna()\n",
    "target = target.rename(columns={'DISCOVERY_DATE': 'DISCOVERY_DATE_julian'})\n",
    "target['DISCOVERY_DATE'] = pd.to_datetime(target['DISCOVERY_DATE_julian'] - pd.Timestamp(0).to_julian_date(), unit='D')\n",
    "target = target.groupby([pd.Grouper(key='DISCOVERY_DATE', freq='1M'),pd.Grouper(key='FIRE_SIZE_CLASS')]).size().unstack(level=1).fillna(0)\n",
    "target.index = target.index.strftime('%Y-%m')    \n",
    "target = target.loc['1992-01':'2015-12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Train/Target : np.array()\n",
    "train = [ df.loc[date].values for date in np.unique(df.index.values)]    \n",
    "x_train, x_test, y_train, y_test = train_test_split(train, target.values, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical Nodes: [11.832160, 56.709788]\n"
     ]
    }
   ],
   "source": [
    "#--- Hidden Layer's Node by Empirical calculation.\n",
    "print('Empirical Nodes: [{:f}, {:f}]'.format(np.sqrt(20*7), np.sqrt(2*(7+1)*201)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "**Deep Learning**, Input + Conv2D + Conv2D + Conv2D + MaxPool2D + Dense x 3 + Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(preprocessing.Normalization(input_shape=[4410,20,1]))\n",
    "    model.add(layers.Conv2D(128,5,strides=1,activation='relu'))\n",
    "    model.add(layers.Conv2D(64,5,strides=1,activation='relu'))    \n",
    "    model.add(layers.Conv2D(32,5,strides=1,activation='relu'))        \n",
    "    model.add(layers.MaxPool2D(3))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='tanh'))\n",
    "    model.add(layers.Dense(32))\n",
    "    model.add(layers.Dense(16))    \n",
    "#     model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32))\n",
    "#     model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(7))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization (None, 4410, 20, 1)       3         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 4406, 16, 128)     3328      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 4402, 12, 64)      204864    \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4398, 8, 32)       51232     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 1466, 2, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 93824)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                6004800   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 6,267,610\n",
      "Trainable params: 6,267,607\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1, model2 = base_model(), base_model()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 - 56s - loss: 44617.4102 - mae: 93.8849 - mse: 44617.4102 - val_loss: 51208.7070 - val_mae: 95.2677 - val_mse: 51208.7070\n",
      "Epoch 2/10\n",
      "5/5 - 65s - loss: 43997.9844 - mae: 93.6952 - mse: 43997.9844 - val_loss: 50718.5781 - val_mae: 94.8659 - val_mse: 50718.5781\n",
      "Epoch 3/10\n",
      "5/5 - 59s - loss: 43480.8086 - mae: 93.2608 - mse: 43480.8086 - val_loss: 50069.5000 - val_mae: 94.3116 - val_mse: 50069.5039\n",
      "Epoch 4/10\n",
      "5/5 - 59s - loss: 42772.6797 - mae: 92.6071 - mse: 42772.6797 - val_loss: 49229.9102 - val_mae: 93.5100 - val_mse: 49229.9102\n",
      "Epoch 5/10\n",
      "5/5 - 61s - loss: 41821.1523 - mae: 91.6195 - mse: 41821.1523 - val_loss: 48156.3125 - val_mae: 92.4185 - val_mse: 48156.3125\n",
      "Epoch 6/10\n",
      "5/5 - 61s - loss: 40641.8984 - mae: 90.2099 - mse: 40641.8984 - val_loss: 46771.2852 - val_mae: 90.9558 - val_mse: 46771.2852\n",
      "Epoch 7/10\n",
      "5/5 - 60s - loss: 39161.8750 - mae: 88.1436 - mse: 39161.8750 - val_loss: 44995.7383 - val_mae: 88.9362 - val_mse: 44995.7383\n",
      "Epoch 8/10\n",
      "5/5 - 59s - loss: 37194.7461 - mae: 85.6880 - mse: 37194.7461 - val_loss: 42796.8789 - val_mae: 86.7388 - val_mse: 42796.8789\n",
      "Epoch 9/10\n",
      "5/5 - 49s - loss: 34981.5117 - mae: 83.3833 - mse: 34981.5117 - val_loss: 40110.4688 - val_mae: 84.2784 - val_mse: 40110.4688\n",
      "Epoch 10/10\n",
      "5/5 - 41s - loss: 32050.0879 - mae: 80.6991 - mse: 32050.0879 - val_loss: 37065.0352 - val_mae: 81.8527 - val_mse: 37065.0352\n"
     ]
    }
   ],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "#-- Optimizer = Adam\n",
    "model1.compile(optimizer='adam',loss='mse',metrics=['mae','mse'])\n",
    "\n",
    "history = model1.fit(np.expand_dims(x_train,axis=-1), \n",
    "                     np.array(y_train), epochs=10, \n",
    "                     validation_split=0.2, \n",
    "                     verbose=2, \n",
    "                     callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 76.4205920105595\n",
      "Mean Squared Error: 30781.42270323913\n",
      "Root Mean Squared Error: 175.44635277838958\n"
     ]
    }
   ],
   "source": [
    "y_pred = model1.predict(np.expand_dims(x_test,axis=-1))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18213.084"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nansum(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because   \n",
    " 1. the nansum(prediction) is equal to 0, this simple conv2d will take more time to tune on it. \n",
    " 2. the dataset is not supporting to the deep learning approach as well.\n",
    " \n",
    "Decide to **stop** here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
